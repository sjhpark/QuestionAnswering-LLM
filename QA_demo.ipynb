{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.llms import GooglePalm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Large Language Model\n",
    "This LLM will be used as a head in a RAG pipeline to generate natural language fashion answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key for the model\n",
    "api_file = open(\"model_API_keys/palm_api.txt\", \"r\")\n",
    "api_key = api_file.readlines()[1] # read second line\n",
    "api_key = api_key.strip() # remove newline (\\n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/envs/QA_llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Pre-trained model\n",
    "model = GooglePalm(google_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(model, question:str):\n",
    "    print(f\"Question: {question}\")\n",
    "    try:\n",
    "        answer = model(question)\n",
    "        print(f\"Answer:\\n{answer}\")\n",
    "    except:\n",
    "        print(\"Similarity search failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 3rd degree burns on palms, what to do?\n",
      "Similarity search failed.\n",
      "Question: What to do if you are tired?\n",
      "Answer:\n",
      "* **Get enough sleep.** This is the most important thing you can do to combat fatigue. Most adults need around 7-8 hours of sleep per night. If you're not getting enough sleep, you're more likely to feel tired during the day.\n",
      "* **Eat a healthy diet.** Eating a healthy diet can help you maintain your energy levels. Make sure to eat plenty of fruits, vegetables, and whole grains. Avoid processed foods and sugary drinks.\n",
      "* **Get regular exercise.** Exercise can help improve your mood and energy levels. Aim for at least 30 minutes of moderate-intensity exercise most days of the week.\n",
      "* **Take breaks throughout the day.** If you're feeling tired, take a break from whatever you're doing and step away for a few minutes. Get some fresh air, stretch, or do something relaxing.\n",
      "* **Avoid caffeine and alcohol.** Caffeine and alcohol can both interfere with sleep, which can lead to fatigue. Avoid consuming them in the hours leading up to bedtime.\n",
      "* **See a doctor if you're concerned.** If you're feeling tired all the time, even after getting enough sleep and making healthy lifestyle changes, you should see a doctor. There may be an underlying medical condition causing your fatigue.\n"
     ]
    }
   ],
   "source": [
    "# Sample questions\n",
    "sample_Q = \"3rd degree burns on palms, what to do?\"\n",
    "get_answer(model, sample_Q)\n",
    "\n",
    "sample_Q = \"What to do if you are tired?\"\n",
    "get_answer(model, sample_Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF Document\n",
    "This model converts natural language texts to embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pdf\n",
    "pdf = os.path.join(\"data\", \"GAN.pdf\") # PDF file\n",
    "loader = PyPDFLoader(pdf) # PDF loader\n",
    "docs = loader.load() # load document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in GAN.pdf: 9\n",
      "5th page: page_content='Theorem 1. The global minimum of the virtual training criterion C(G)is achieved if and only if\\npg=pdata. At that point, C(G)achieves the value −log 4 .\\nProof. Forpg=pdata,D∗\\nG(x) =1\\n2, (consider Eq. 2). Hence, by inspecting Eq. 4 at D∗\\nG(x) =1\\n2, we\\nﬁndC(G) = log1\\n2+ log1\\n2=−log 4 . To see that this is the best possible value of C(G), reached\\nonly forpg=pdata, observe that\\nEx∼pdata[−log 2] + Ex∼pg[−log 2] =−log 4\\nand that by subtracting this expression from C(G) =V(D∗\\nG,G), we obtain:\\nC(G) =−log(4) +KL(\\npdata\\ued79\\ued79\\ued79\\ued79pdata+pg\\n2)\\n+KL(\\npg\\ued79\\ued79\\ued79\\ued79pdata+pg\\n2)\\n(5)\\nwhere KL is the Kullback–Leibler divergence. We recognize in the previous expression the Jensen–\\nShannon divergence between the model’s distribution and the data generating process:\\nC(G) =−log(4) + 2·JSD (pdata∥pg) (6)\\nSince the Jensen–Shannon divergence between two distributions is always non-negative and zero\\nonly when they are equal, we have shown that C∗=−log(4) is the global minimum of C(G)and\\nthat the only solution is pg=pdata, i.e., the generative model perfectly replicating the data generating\\nprocess.\\n4.2 Convergence of Algorithm 1\\nProposition 2. IfGandDhave enough capacity, and at each step of Algorithm 1, the discriminator\\nis allowed to reach its optimum given G, andpgis updated so as to improve the criterion\\nEx∼pdata[logD∗\\nG(x)] +Ex∼pg[log(1−D∗\\nG(x))]\\nthenpgconverges to pdata\\nProof. ConsiderV(G,D ) =U(pg,D)as a function of pgas done in the above criterion. Note\\nthatU(pg,D)is convex in pg. The subderivatives of a supremum of convex functions include the\\nderivative of the function at the point where the maximum is attained. In other words, if f(x) =\\nsupα∈Afα(x)andfα(x)is convex in xfor everyα, then∂fβ(x)∈∂fifβ= arg supα∈Afα(x).\\nThis is equivalent to computing a gradient descent update for pgat the optimal Dgiven the cor-\\nrespondingG.supDU(pg,D)is convex in pgwith a unique global optima as proven in Thm 1,\\ntherefore with sufﬁciently small updates of pg,pgconverges to px, concluding the proof.\\nIn practice, adversarial nets represent a limited family of pgdistributions via the function G(z;θg),\\nand we optimize θgrather thanpgitself. Using a multilayer perceptron to deﬁne Gintroduces\\nmultiple critical points in parameter space. However, the excellent performance of multilayer per-\\nceptrons in practice suggests that they are a reasonable model to use despite their lack of theoretical\\nguarantees.\\n5 Experiments\\nWe trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database\\n(TFD) [28], and CIFAR-10 [21]. The generator nets used a mixture of rectiﬁer linear activations [19,\\n9] and sigmoid activations, while the discriminator net used maxout [10] activations. Dropout [17]\\nwas applied in training the discriminator net. While our theoretical framework permits the use of\\ndropout and other noise at intermediate layers of the generator, we used noise as the input to only\\nthe bottommost layer of the generator network.\\nWe estimate probability of the test set data under pgby ﬁtting a Gaussian Parzen window to the\\nsamples generated with Gand reporting the log-likelihood under this distribution. The σparameter\\n5' metadata={'source': 'data/GAN.pdf', 'page': 4}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of pages in {os.path.basename(pdf)}: {len(docs)}\")\n",
    "print(f\"5th page: {docs[4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, TokenTextSplitter, NLTKTextSplitter, SpacyTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Cassandra, Chroma, FAISS # vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Splitting: split the document into small chunks\n",
    "chunk_size = 500\n",
    "text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0, separator=\"\\n\")\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained Text Embedding Model\n",
    "embedding_model = HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# Embedding vectors\n",
    "device = \"cuda\"\n",
    "query_instruction = \"Represent the query for retrieval: \"\n",
    "embeddings = embedding_model(query_instruction=query_instruction,\n",
    "                             model_kwargs={\"device\": device},\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector database (options: Cassandra, Chroma, FAISS, etc.)\n",
    "db = FAISS.from_documents(documents=chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector retrieval from database\n",
    "search_type = \"similarity\"\n",
    "k = 3 # top k similar documents\n",
    "retriever = db.as_retriever(\n",
    "    search_type=search_type,\n",
    "    search_kwargs={\"k\": k}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Adversarial Nets\n",
      "Ian J. Goodfellow, Jean Pouget-Abadie∗, Mehdi Mirza, Bing Xu, David Warde-Farley,\n",
      "Sherjil Ozair†, Aaron Courville, Yoshua Bengio‡\n",
      "D´epartement d’informatique et de recherche op ´erationnelle\n",
      "Universit ´e de Montr ´eal\n",
      "Montr ´eal, QC H3C 3J7\n",
      "Abstract\n",
      "We propose a new framework for estimating generative models via an adversar-\n",
      "ial process, in which we simultaneously train two models: a generative model G\n",
      "that captures the data distribution, and a discriminative model Dthat estimates\n",
      "the probability that a sample came from the training data rather than G. The train-\n",
      "ing procedure for Gis to maximize the probability of Dmaking a mistake. This\n",
      "framework corresponds to a minimax two-player game. In the space of arbitrary\n",
      "functionsGandD, a unique solution exists, with Grecovering the training data\n",
      "distribution and Dequal to1\n",
      "2everywhere. In the case where GandDare deﬁned\n",
      "by multilayer perceptrons, the entire system can be trained with backpropagation.\n",
      "There is no need for any Markov chains or unrolled approximate inference net-\n",
      "works during either training or generation of samples. Experiments demonstrate\n",
      "the potential of the framework through qualitative and quantitative evaluation of\n",
      "the generated samples.\n",
      "1 Introduction\n",
      "The promise of deep learning is to discover rich, hierarchical models [2] that represent probability\n",
      "distributions over the kinds of data encountered in artiﬁcial intelligence applications, such as natural\n",
      "images, audio waveforms containing speech, and symbols in natural language corpora. So far, the\n",
      "most striking successes in deep learning have involved discriminative models, usually those that\n",
      "map a high-dimensional, rich sensory input to a class label [14, 22]. These striking successes have\n",
      "primarily been based on the backpropagation and dropout algorithms, using piecewise linear units\n",
      "[19, 9, 10] which have a particularly well-behaved gradient . Deep generative models have had less\n",
      "of an impact, due to the difﬁculty of approximating many intractable probabilistic computations that\n",
      "arise in maximum likelihood estimation and related strategies, and due to difﬁculty of leveraging\n",
      "the beneﬁts of piecewise linear units in the generative context. We propose a new generative model\n",
      "estimation procedure that sidesteps these difﬁculties.1\n",
      "In the proposed adversarial nets framework, the generative model is pitted against an adversary: a\n",
      "discriminative model that learns to determine whether a sample is from the model distribution or the\n",
      "data distribution. The generative model can be thought of as analogous to a team of counterfeiters,\n",
      "trying to produce fake currency and use it without detection, while the discriminative model is\n",
      "analogous to the police, trying to detect the counterfeit currency. Competition in this game drives\n",
      "both teams to improve their methods until the counterfeits are indistiguishable from the genuine\n",
      "articles.\n",
      "∗Jean Pouget-Abadie is visiting Universit ´e de Montr ´eal from Ecole Polytechnique.\n",
      "†Sherjil Ozair is visiting Universit ´e de Montr ´eal from Indian Institute of Technology Delhi\n",
      "‡Yoshua Bengio is a CIFAR Senior Fellow.\n",
      "1All code and hyperparameters available at http://www.github.com/goodfeli/adversarial\n",
      "1arXiv:1406.2661v1  [stat.ML]  10 Jun 2014\n"
     ]
    }
   ],
   "source": [
    "# show top k similar texts retrieved from the vector database\n",
    "similar_texts = retriever.get_relevant_documents(\"Adversarial modeling\")\n",
    "print(similar_texts[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q&A chain\n",
    "QA_chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\", # other options: map_reduce, refine, etc.\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What does the paper propose?\n",
      "Answer: The paper proposes a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model Dthat estimates the probability that a sample came from the training data rather than G. The training procedure for Gis to maximize the probability of Dmaking a mistake.\n",
      "Question: Who are the authors?\n",
      "Answer: Ian J. Goodfellow, Jean Pouget-Abadie∗, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair†, Aaron Courville, Yoshua Bengio\n"
     ]
    }
   ],
   "source": [
    "# Ask questions\n",
    "query = \"What does the paper propose?\"\n",
    "response = QA_chain(query)\n",
    "print(f\"Question: {query}\\nAnswer: {response['result']}\")\n",
    "\n",
    "query = \"Who are the authors?\"\n",
    "response = QA_chain(query)\n",
    "print(f\"Question: {query}\\nAnswer: {response['result']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QA_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
